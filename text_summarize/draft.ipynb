{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\n",
    "    'webpage-1': set(['webpage-2', 'webpage-4', 'webpage-5', 'webpage-6', 'webpage-8', 'webpage-9', 'webpage-10']),\n",
    "    'webpage-2': set(['webpage-5', 'webpage-6']),\n",
    "    'webpage-3': set(['webpage-10']),\n",
    "    'webpage-4': set(['webpage-9']),\n",
    "    'webpage-5': set(['webpage-2', 'webpage-4']),\n",
    "    'webpage-6': set([]), # dangling page\n",
    "    'webpage-7': set(['webpage-1', 'webpage-3', 'webpage-4']),\n",
    "    'webpage-8': set(['webpage-1']),\n",
    "    'webpage-9': set(['webpage-1', 'webpage-2', 'webpage-3', 'webpage-8', 'webpage-10']),\n",
    "    'webpage-10': set(['webpage-2', 'webpage-3', 'webpage-8', 'webpage-9']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idx(links):\n",
    "    return {web:str(idx) for idx, web in enumerate(list(links.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'webpage-1': '0',\n",
       " 'webpage-2': '1',\n",
       " 'webpage-3': '2',\n",
       " 'webpage-4': '3',\n",
       " 'webpage-5': '4',\n",
       " 'webpage-6': '5',\n",
       " 'webpage-7': '6',\n",
       " 'webpage-8': '7',\n",
       " 'webpage-9': '8',\n",
       " 'webpage-10': '9'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_idx(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(links):\n",
    "    links_idx = build_idx(links)\n",
    "    graph = {}\n",
    "    \n",
    "    for key, val in links.items():\n",
    "        graph[links_idx[key]] = [links_idx[v] for v in val]\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['8', '3', '1', '4', '7', '5', '9'],\n",
       " '1': ['5', '4'],\n",
       " '2': ['9'],\n",
       " '3': ['8'],\n",
       " '4': ['3', '1'],\n",
       " '5': [],\n",
       " '6': ['3', '2', '0'],\n",
       " '7': ['0'],\n",
       " '8': ['2', '1', '7', '9', '0'],\n",
       " '9': ['7', '8', '2', '1']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = build_graph(links)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(graph):\n",
    "    n = len(graph)\n",
    "    matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        if not graph[str(i)]:\n",
    "            matrix[i,:] = np.ones((1, n)) * 1 / n\n",
    "        \n",
    "        for j in graph[str(i)]:\n",
    "            matrix[i][int(j)] = 1 / len(graph[str(i)])\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.14285714, 0.        , 0.14285714, 0.14285714,\n",
       "        0.14285714, 0.        , 0.14285714, 0.14285714, 0.14285714],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.5       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.33333333, 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2       , 0.        , 0.2       ],\n",
       "       [0.        , 0.25      , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25      , 0.25      , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = build_matrix(graph)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pangrank(matrix, eps = 1.0e-8, d = 0.85):\n",
    "    \n",
    "    n = matrix.shape[0]\n",
    "    R = np.ones((n, 1)) * 1 / n\n",
    "    new_R = np.zeros((n, 1))\n",
    "    E = np.ones((n, n))\n",
    "\n",
    "    while True:\n",
    "        new_R = ((1 - d) / n * E + d * matrix.T).dot(R)\n",
    "        if np.linalg.norm(new_R - R) <= eps:\n",
    "            break\n",
    "        else:\n",
    "            R = new_R\n",
    "    \n",
    "    return new_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13009588],\n",
       "       [0.13050742],\n",
       "       [0.08116303],\n",
       "       [0.08539887],\n",
       "       [0.09427651],\n",
       "       [0.09427651],\n",
       "       [0.0230135 ],\n",
       "       [0.0904399 ],\n",
       "       [0.13934097],\n",
       "       [0.13148741]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pangrank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0.1300956900118335,\n",
       " '1': 0.13050776900162686,\n",
       " '2': 0.08116315448394983,\n",
       " '3': 0.08539910029620076,\n",
       " '4': 0.09427630133632156,\n",
       " '5': 0.09427630133632156,\n",
       " '6': 0.023013537818899983,\n",
       " '7': 0.09044007990712699,\n",
       " '8': 0.1393406839069631,\n",
       " '9': 0.13148738190075582}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#G = nx.DiGraph(nx.path_graph(4))\n",
    "pr = nx.pagerank(g)\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"中新网8月3日电 据美国《世界日报》报道，在杀害女儿章莹颖的凶手克里斯滕森判处终身监禁后，章父曾心痛且近乎绝望地表示，希望被告无条件地告知章莹颖的下落，“请帮助结束我们的煎熬，让我们带莹颖回家”，2日虽然传出章的遗体丢在伊利诺伊州一处垃圾掩埋场，然而事发至今已逾两年，要真正找到莹颖遗体，恐怕也非易事。\\\n",
    "\\\n",
    "位于伊州中部的沃米利安郡境内，共有两处由“共和服务公司”(Republic Services)经营的垃圾掩埋场，目前还未确定调查人员是否已经展开搜索，垃圾掩埋场发言人2日表示，如有需要，他们会全力配合调查。\\\n",
    "\\\n",
    "共和服务公司在该郡的掩埋场，根据2014年记录，分别是每年可处理约22.8万吨垃圾的Brickyard Disposal Landfill，及每年可处理约4.1万吨垃圾的Illinois Landfill。\\\n",
    "\\\n",
    "要在过去两年累积数十万吨的“垃圾海”中，挖到章的遗骇，难度可想而知。\\\n",
    "\\\n",
    "另一方面，根据贝克特声明，检察官在告诉家人此讯息时，也提到克里斯滕森的辩护团队是在获得豁免的情况下，向政府提供了上述信息。\\\n",
    "\\\n",
    "贝克特说，章家人并没有收到这项消息是否真实的保证，这种情形也增加了“搜索垃圾场”可能又是一场空的可能性。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"中新网8月3日电 据美国《世界日报》报道，在杀害女儿章莹颖的凶手克里斯滕森判处终身监禁后，章父曾心痛且近乎绝望地表示，希望被告无条件地告知章莹颖的下落，“请帮助结束我们的煎熬，让我们带莹颖回家”，2日虽然传出章的遗体丢在伊利诺伊州一处垃圾掩埋场，然而事发至今已逾两年，要真正找到莹颖遗体，恐怕也非易事。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, [4, 5]]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a.append([4,5])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyWord:\n",
    "    def __init__(self, win_size=10, words=20):\n",
    "        self.news = None\n",
    "        self.win_size = win_size\n",
    "        self.words = words\n",
    "        self.stopwords = []\n",
    "        try:\n",
    "            with open('stopword.txt', 'r') as f:\n",
    "                self.stopwords = f.read().splitlines()\n",
    "        except:\n",
    "            print(\"stopword.txt not exist\")\n",
    "            self.stopwords = []\n",
    "    \n",
    "    def __build_graph(self, tokens):\n",
    "        graph = {}\n",
    "        _m = len(tokens)\n",
    "\n",
    "        for i in range(_m):\n",
    "            left = i - self.win_size\n",
    "            right = i + self.win_size\n",
    "            if left < 0: left = 0\n",
    "            if right > _m: right = _m\n",
    "            \n",
    "            graph[tokens[i]] = tokens[left:i]\n",
    "            graph[tokens[i]] += tokens[i:right]\n",
    "\n",
    "\n",
    "        g = nx.DiGraph(graph, cmap = plt.get_cmap('jet'),)\n",
    "\n",
    "        pr = nx.pagerank(g)\n",
    "        \n",
    "        return pr\n",
    "    \n",
    "    def build_tokens(self, news):\n",
    "        clean_news = ' '.join(re.findall(re.compile('[\\w|\\d]+'), news))\n",
    "        \n",
    "        tokens = ' '.join(jieba.cut(clean_news)).split()\n",
    "        \n",
    "        tokens = [t for t in tokens if t not in self.stopwords]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def extract(self, news):\n",
    "        \n",
    "        tokens = self.build_tokens(news)\n",
    "        \n",
    "        #print(tokens)\n",
    "        \n",
    "        pr = self.__build_graph(tokens)\n",
    "\n",
    "        pr = sorted(pr.items(), key=lambda x: x[1], reverse=True)\n",
    "        return pr\n",
    "        return [_pr[0] for _pr in pr][:self.words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = KeyWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ' '.join(keyword.build_tokens(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('章', 0.024803697655102607),\n",
       " ('垃圾', 0.022606805914234904),\n",
       " ('贝克特', 0.020298222867835152),\n",
       " ('家人', 0.019937787136035966),\n",
       " ('克里斯滕森', 0.015235677030760246),\n",
       " ('搜索', 0.014215865414227761),\n",
       " ('收到', 0.014175296427624936),\n",
       " ('说', 0.014087681994751942),\n",
       " ('信息', 0.01376277836279981),\n",
       " ('这项', 0.013610993224916518),\n",
       " ('提供', 0.013590630130711896),\n",
       " ('两年', 0.013489435051990337),\n",
       " ('政府', 0.013399630389942291),\n",
       " ('万吨', 0.013092281820151772),\n",
       " ('约', 0.01305141500761649),\n",
       " ('消息', 0.013037903543312822),\n",
       " ('掩埋场', 0.012945686276560632),\n",
       " ('下', 0.012812352715638266),\n",
       " ('情况', 0.012615872328493859),\n",
       " ('真实', 0.012438546473740719),\n",
       " ('豁免', 0.01241860469581851),\n",
       " ('团队', 0.012216373991244043),\n",
       " ('Landfill', 0.012155982238770324),\n",
       " ('保证', 0.01186104229641508),\n",
       " ('辩护', 0.011369146021500402),\n",
       " ('情形', 0.0112762240253577),\n",
       " ('8', 0.011172760760141657),\n",
       " ('服务公司', 0.010592528604897596),\n",
       " ('共和', 0.010527487180488721),\n",
       " ('提到', 0.010509967381367995),\n",
       " ('增加', 0.010413653567059308),\n",
       " ('数十万吨', 0.010345772604603282),\n",
       " ('1', 0.010236402815104266),\n",
       " ('Illinois', 0.01010626379333278),\n",
       " ('时', 0.010043766491025558),\n",
       " ('累积', 0.009945656911999971),\n",
       " ('日', 0.009870982712998536),\n",
       " ('海', 0.009863521149193144),\n",
       " ('4', 0.009821068487684059),\n",
       " ('2', 0.009778834618956614),\n",
       " ('讯息', 0.009458948219968176),\n",
       " ('声明', 0.009414972852167167),\n",
       " ('难度', 0.0093998602055755),\n",
       " ('遗骇', 0.009326185443616342),\n",
       " ('中', 0.009283387029403319),\n",
       " ('可想而知', 0.009271347799395853),\n",
       " ('调查', 0.009165147510317063),\n",
       " ('告诉', 0.009099610288329799),\n",
       " ('Brickyard', 0.009019092984007488),\n",
       " ('挖到', 0.008989725187872975),\n",
       " ('检察官', 0.008967812263434653),\n",
       " ('22', 0.008962919028110314),\n",
       " ('Disposal', 0.008938540247799514),\n",
       " ('垃圾场', 0.008760955155393985),\n",
       " ('章莹颖', 0.008386632911728488),\n",
       " ('记录', 0.008371582755664981),\n",
       " ('郡', 0.008284697867685323),\n",
       " ('年', 0.008162546282917005),\n",
       " ('2014', 0.00793527750914332),\n",
       " ('一场空', 0.007913727185650343),\n",
       " ('配合', 0.007528683985104347),\n",
       " ('会', 0.0073591464228456),\n",
       " ('全力', 0.0073591464228456),\n",
       " ('可能性', 0.007311380788472567),\n",
       " ('发言人', 0.006706129587930766),\n",
       " ('遗体', 0.00592130035139005),\n",
       " ('希望', 0.005908741644544406),\n",
       " ('近乎', 0.005800653090605231),\n",
       " ('凶手', 0.005766925961948662),\n",
       " ('后', 0.005757436640749834),\n",
       " ('绝望', 0.005737478369315638),\n",
       " ('判处', 0.005719947312324788),\n",
       " ('章父', 0.005710176568571182),\n",
       " ('终身', 0.005687915906749655),\n",
       " ('被告', 0.005665625358794788),\n",
       " ('曾', 0.00566049461869038),\n",
       " ('监禁', 0.005656494327984951),\n",
       " ('心痛', 0.005608350081522481),\n",
       " ('女儿', 0.005569074880785604),\n",
       " ('无条件', 0.0054238703645739204),\n",
       " ('告知', 0.005346961283051134),\n",
       " ('杀害', 0.005318155498894217),\n",
       " ('展开', 0.005311738506284262),\n",
       " ('报道', 0.005064903043877699),\n",
       " ('人员', 0.0049016657890724215),\n",
       " ('下落', 0.004846775735267974),\n",
       " ('世界日报', 0.004809427744193871),\n",
       " ('请', 0.004740167599599929),\n",
       " ('结束', 0.004635370896866471),\n",
       " ('美国', 0.004564717496094539),\n",
       " ('恐怕', 0.004538266179424023),\n",
       " ('非', 0.004538266179424023),\n",
       " ('煎熬', 0.004524493302137976),\n",
       " ('未确定', 0.004383131403729395),\n",
       " ('易事', 0.004372027623638404),\n",
       " ('位于', 0.004359910221176047),\n",
       " ('莹颖', 0.0043469539203491794),\n",
       " ('伊州', 0.004339019731842769),\n",
       " ('日电', 0.00432429792433562),\n",
       " ('找到', 0.004317100869603239),\n",
       " ('中部', 0.004309874469630036),\n",
       " ('沃米', 0.004309874469630036),\n",
       " ('带莹颖', 0.004280631666860767),\n",
       " ('还', 0.004209648922768501),\n",
       " ('境内', 0.004140445693499337),\n",
       " ('利安郡', 0.004126391865519642),\n",
       " ('逾', 0.004105191074443247),\n",
       " ('共有', 0.004095779541816331),\n",
       " ('3', 0.004082542930114752),\n",
       " ('回家', 0.004029490789355789),\n",
       " ('已', 0.003922014849918927),\n",
       " ('两处', 0.0039028968211969016),\n",
       " ('至今', 0.003901110568682303),\n",
       " ('传出', 0.003847120253172309),\n",
       " ('月', 0.0038394266443651337),\n",
       " ('Republic', 0.0038212440774115118),\n",
       " ('事发', 0.0037158080970386147),\n",
       " ('Services', 0.0036368293059387546),\n",
       " ('中新网', 0.003594314081598262),\n",
       " ('一处', 0.00349742647153568),\n",
       " ('经营', 0.0034536530814144326),\n",
       " ('丢', 0.0034503282763910497),\n",
       " ('伊利诺伊州', 0.0034380635704940044)]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.extract(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers.\\\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict\\\n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\\\n",
    "components of a minimal set of solutions and algorithms of construction of\\\n",
    "minimal generating sets of solutions for all types of systems are given.\\\n",
    "These criteria and the corresponding algorithms for constructing a minimal\\\n",
    "supporting set of solutions can be used in solving all the considered types\\\n",
    "systems and systems of mixed types.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-0bbe4ae58eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'keywords' is not defined"
     ]
    }
   ],
   "source": [
    "print(keywords.keywords(a,words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from scipy.sparse import linalg \n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"sqlResult_1558435.csv\", encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = database['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_char(corpus):\n",
    "    return [' '.join(re.findall(re.compile('[\\w|\\d]+'), str(cor))) for cor in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = remove_extra_char(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuts(corpus):\n",
    "    jieba.enable_parallel(4)\n",
    "    \n",
    "    tokens = [' '.join(jieba.cut(cor)) for cor in corpus]\n",
    "    \n",
    "    jieba.disable_parallel()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = cuts(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_strip = [t.split() for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for t in tokens_strip:\n",
    "    all_tokens += t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['原', '标题']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "del counter['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(counter):\n",
    "    sum_words = 0\n",
    "    with open('counter.txt', 'w') as f:\n",
    "        for k, v in counter.items():\n",
    "            f.write('{}:{}\\n'.format(k, v))\n",
    "\n",
    "    for k, v in counter.items():\n",
    "        sum_words += int(v)\n",
    "\n",
    "    with open('sum_count.txt', 'w') as f:\n",
    "        f.write(str(sum_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "\n",
    "    with open('counter.txt', 'r') as f:\n",
    "        lines = [line.strip().split(':') for line in f]\n",
    "\n",
    "    counter = Counter({line[0]: int(line[1]) for line in lines})\n",
    "\n",
    "    with open('sum_count.txt', 'r') as f:\n",
    "        sum_words = int(f.read())\n",
    "    \n",
    "    return counter, sum_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter, sum_words = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 493316),\n",
       " ('在', 204730),\n",
       " ('月', 164161),\n",
       " ('日', 150946),\n",
       " ('新华社', 134333),\n",
       " ('年', 105294),\n",
       " ('和', 97337),\n",
       " ('１', 87073),\n",
       " ('０', 83492),\n",
       " ('外代', 83266)]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenVec:\n",
    "    def __init__(self, alpha=0.0001):\n",
    "        self.alpha = alpha\n",
    "        self.model, self.norm_vector = self.__load_vector()\n",
    "        self.counter, self.sum_words = self.__load_prob()\n",
    "        self.sens = None\n",
    "        self.stopwords = self.__load_stopwords()\n",
    "        \n",
    "    def __load_stopwords(self):\n",
    "        stopwords = []\n",
    "        try:\n",
    "            with open('stopword.txt', 'r') as f:\n",
    "                stopwords = f.read().splitlines()\n",
    "        except:\n",
    "            print(\"stopword.txt not exist\")\n",
    "            stopwords = []\n",
    "            \n",
    "        return stopwords\n",
    "\n",
    "    \n",
    "    def __load_vector(self):\n",
    "        try:\n",
    "            model = Word2Vec.load(\"vector\")\n",
    "            words_dict = dict({})\n",
    "            for idx, key in enumerate(model.wv.vocab):\n",
    "                words_dict[key] = model.wv[key]\n",
    "            \n",
    "            all_embs = np.stack(words_dict.values())\n",
    "            vector_size = all_embs.shape[1]\n",
    "            \n",
    "            emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "            \n",
    "            norm_vector = np.random.normal(emb_mean, emb_std, (1, vector_size))   \n",
    "        except:\n",
    "            print(\"error when load vector model\")\n",
    "            model = None\n",
    "            norm_vector = None\n",
    "            \n",
    "        return model, norm_vector\n",
    "    \n",
    "    def __word_vector(self, word):\n",
    "        try:\n",
    "            return self.mode.wv[word]\n",
    "        except:\n",
    "            return self.norm_vector\n",
    "    \n",
    "    def __load_prob(self):\n",
    "        try:\n",
    "            with open('counter.txt', 'r') as f:\n",
    "                lines = [line.strip().split(':') for line in f]\n",
    "\n",
    "            counter = Counter({line[0]: int(line[1]) for line in lines})\n",
    "\n",
    "            with open('sum_count.txt', 'r') as f:\n",
    "                sum_words = int(f.read())\n",
    "        except:\n",
    "            print(\"error when load prob\")\n",
    "            counter = None\n",
    "            sum_words = 1\n",
    "    \n",
    "        return counter, sum_words\n",
    "    \n",
    "    def __word_prob(self, word):\n",
    "        if word in self.counter:\n",
    "            return self.counter[word] / self.sum_words\n",
    "        else:\n",
    "            return 1 / self.sum_words\n",
    "    \n",
    "    def __cut_to_sen(self, sentences):\n",
    "        marks = \"，|。|？|！|；|：|,|\\.|!|\\?|:|\\\"\"\n",
    "        sens = [sen for sen in re.split(marks, sentences) if sen]\n",
    "        return sens\n",
    "    \n",
    "        \n",
    "    def __cut_to_tokens(self, sen):\n",
    "        clean_sen = ' '.join(re.findall(re.compile('[\\w|\\d]+'), sen))\n",
    "        \n",
    "        tokens = ' '.join(jieba.cut(clean_sen)).split()\n",
    "        \n",
    "        tokens = [t for t in tokens if t not in self.stopwords]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def __calc(self, word):\n",
    "        return self.alpha / (self.alpha + self.__word_prob(word)) * self.__word_vector(word)\n",
    "    \n",
    "    def __vector(self, sens):\n",
    "        sen_vec = []\n",
    "        for sen in sens:\n",
    "            if not sen: continue\n",
    "                \n",
    "            words = self.__cut_to_tokens(sen)\n",
    "            \n",
    "            vector_s = [self.__calc(word) for word in words if word]\n",
    "                \n",
    "            vector_s = 1 / len(words) * sum(vector_s)\n",
    "            \n",
    "            sen_vec.append(vector_s)\n",
    "\n",
    "        #if not define v0, u will be random\n",
    "        _sen_vec = np.array(sen_vec).reshape(len(sen_vec), -1).T\n",
    "        v0 = np.ones(min(_sen_vec.shape))\n",
    "        u, s, v = linalg.svds(_sen_vec, k=1, v0=v0)\n",
    "        #u = np.around(u, decimals=12)\n",
    "\n",
    "        res = []\n",
    "        for vec in sen_vec:\n",
    "            res.append(vec.T - np.dot(np.dot(u, u.T), vec.T))\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def cosine(self, vec1, vec2):\n",
    "        return 1 - cosine(vec1, vec2)\n",
    "    \n",
    "    def get_sens(self):\n",
    "        return self.sens \n",
    "    \n",
    "    def get_vector(self, sentences):\n",
    "        self.sens = self.__cut_to_sen(sentences)\n",
    "\n",
    "        return self.__vector(self.sens)\n",
    "\n",
    "    def pangrank(self, matrix, eps = 1.0e-8, d = 0.85):\n",
    "\n",
    "        n = matrix.shape[0]\n",
    "        R = np.ones((n, 1)) * 1 / n\n",
    "        new_R = np.zeros((n, 1))\n",
    "        E = np.ones((n, n))\n",
    "\n",
    "        while True:\n",
    "            new_R = ((1 - d) / n * E + d * matrix.T).dot(R)\n",
    "            if np.linalg.norm(new_R - R) <= eps:\n",
    "                break\n",
    "            else:\n",
    "                R = new_R\n",
    "\n",
    "        return new_R    \n",
    "    \n",
    "    def textrank(self, sentences):\n",
    "        sen_vec = self.get_vector(sentences)\n",
    "\n",
    "        n = len(sen_vec)\n",
    "        S = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j: continue\n",
    "                \n",
    "                S[i][j] = self.cosine(sen_vec[i], sen_vec[j])\n",
    "        \n",
    "        for i in range(n):\n",
    "            S[i] /= S[i].sum()\n",
    "        \n",
    "        return self.pangrank(S)\n",
    "    \n",
    "    def summarization(self, sentences, count=5):\n",
    "        sens = self.__cut_to_sen(sentences)\n",
    "        rank = self.textrank(sentences)\n",
    "        \n",
    "        rank = dict(zip(sens, rank))\n",
    "\n",
    "        rank = sorted(rank.items(), key=lambda x: x[1], reverse=True)\n",
    "        #print(rank)\n",
    "        return [r[0] for r in rank]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "senvec = SenVec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vec = senvec.get_vector(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“请帮助结束我们的煎熬',\n",
       " '恐怕也非易事',\n",
       " '让我们带莹颖回家”',\n",
       " '要真正找到莹颖遗体',\n",
       " '然而事发至今已逾两年',\n",
       " '章父曾心痛且近乎绝望地表示',\n",
       " '2日虽然传出章的遗体丢在伊利诺伊州一处垃圾掩埋场',\n",
       " '在杀害女儿章莹颖的凶手克里斯滕森判处终身监禁后',\n",
       " '希望被告无条件地告知章莹颖的下落',\n",
       " '中新网8月3日电 据美国《世界日报》报道']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senvec.summarization(news, count=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
