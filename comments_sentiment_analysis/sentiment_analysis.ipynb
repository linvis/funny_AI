{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./ai_challenger_sentiment_analysis_trainingset_20180816/sentiment_analysis_trainingset.csv')\n",
    "valid = pd.read_csv('./ai_challenger_sentiment_analysis_validationset_20180816/sentiment_analysis_validationset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'content', 'location_traffic_convenience',\n",
       "       'location_distance_from_business_district', 'location_easy_to_find',\n",
       "       'service_wait_time', 'service_waiters_attitude',\n",
       "       'service_parking_convenience', 'service_serving_speed', 'price_level',\n",
       "       'price_cost_effective', 'price_discount', 'environment_decoration',\n",
       "       'environment_noise', 'environment_space', 'environment_cleaness',\n",
       "       'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation',\n",
       "       'others_overall_experience', 'others_willing_to_consume_again'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_item = [\n",
    "    'location_traffic_convenience', \n",
    "    'location_distance_from_business_district', \n",
    "    'location_easy_to_find'\n",
    "]\n",
    "service_item = [\n",
    "    'service_wait_time', \n",
    "    'service_waiters_attitude', \n",
    "    'service_parking_convenience',\n",
    "    'service_serving_speed'\n",
    "]\n",
    "price_item = [\n",
    "    'price_level', \n",
    "    'price_cost_effective', \n",
    "    'price_discount'\n",
    "]\n",
    "environment_item = [\n",
    "    'environment_decoration',\n",
    "    'environment_noise',\n",
    "    'environment_space',\n",
    "    'environment_cleaness',\n",
    "]\n",
    "dish_item = [\n",
    "    'dish_portion',\n",
    "    'dish_taste',\n",
    "    'dish_look',\n",
    "    'dish_recommendation',\n",
    "]\n",
    "others_item = [\n",
    "    'others_overall_experience',\n",
    "    'others_willing_to_consume_again'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarse_y(database):\n",
    "    items = [location_item, service_item, price_item, environment_item, dish_item, others_item]\n",
    "    coarse_y = []\n",
    "    \n",
    "    for item in items:\n",
    "        item_y = [0 if sum(score) == -6 else 1 for score in list(database[item].values)]\n",
    "        coarse_y.append(item_y)\n",
    "    \n",
    "    return np.array(coarse_y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = get_coarse_y(train)\n",
    "valid_y = get_coarse_y(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = train['content'].tolist()\n",
    "valid_content = valid['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_token(sentences):\n",
    "    # remove extra character\n",
    "    clean_sentences = [' '.join(re.findall(re.compile('[\\w|\\d|]+'), sen)) for sen in sentences]\n",
    "    \n",
    "    jieba.enable_parallel(8)\n",
    "    \n",
    "    tokens = [' '.join(jieba.cut(sen)) for sen in clean_sentences]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.827 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-9:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/lin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_tokens = sentence_to_token(train_content)\n",
    "valid_tokens = sentence_to_token(valid_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吼吼 吼   萌死 人 的 棒棒糖   中 了 大众 点评 的 霸王餐   太 可爱 了   一直 就 好奇 这个 棒棒糖 是 怎么 个 东西   大众 点评 给 了 我 这个 土老冒 一个 见识 的 机会   看 介绍 棒棒糖 是 用 德国 糖 做 的   不会 很甜   中间 的 照片 是 糯米 的   能 食用   真是太 高端 大气 上档次 了   还 可以 买 蝴蝶结 扎口   送人 可以 买 礼盒   我 是 先 打 的 卖家 电话   加 了 微信   给 卖家 传 的 照片   等 了 几天   卖家 就 告诉 我 可以 取货 了   去 大官 屯 那取 的   虽然 连 卖家 的 面 都 没 见到   但是 还是 谢谢 卖家 送 我 这么 可爱 的 东西   太 喜欢 了   这 哪 舍得吃 啊',\n",
       " '第三次 参加 大众 点评 网 霸王餐 的 活动   这家 店 给 人 整体 感觉 一般   首先 环境 只能 算 中等   其次 霸王餐 提供 的 菜品 也 不是 很多   当然 商家 为了 避免 参加 霸王餐 吃不饱 的 现象   给 每桌 都 提供 了 至少 六份 主食   我们 那桌 都 提供 了 两份 年糕   第一次 吃火锅 会 在 桌上 有 这么 多 的 主食 了   整体 来说 这家 火锅店 没有 什么 特别 有 特色 的   不过 每份 菜品 分量 还是 比较 足 的   这点 要 肯定   至于 价格   因为 没有 看 菜单 不 了解   不过 我 看 大众 有 这家 店 的 团购 代金券   相当于 7 折   应该 价位 不会 很 高 的   最后 还是 要 感谢 商家 提供 霸王餐   祝 生意兴隆   财源 广进',\n",
       " '4 人 同行   点 了 10 个 小吃   榴莲 酥   榴莲 味道 不足   松软   奶味 浓   虾饺   好吃   两颗 大 虾仁   皮蛋 粥   皮蛋 多   但是 一般   挺 稠 的   奶黄包   很 好吃   真的 是 蛋黄 和 奶   而且 真的 是 流沙   叉烧包   面香   鲜虾 烧卖   好吃   外面 的 黄色 皮 看着 让 人 特别 有 食欲   云吞面   云吞 分量 足   但是 汤头 不是 很 好喝   而且 云吞 的 馅儿 不知 为何 感觉 不是 很 新鲜   鲍汁 腐皮卷   没 怎么 吃   味道 倒 是 不错   排骨   味道 不错   不算 很腻   但是 油 确实 微多   鲜虾 锅贴   确实 今天 吃 了 很多 虾   这个 很 酥脆   里头 的 虾 也 很 好吃   刚好 有 优惠券   所以 4 个人 花 了 100 不到   这个 优惠券 只能 在 1 层用   5 层用 不了   原价 大概 人均 50   服务 一般   上菜 速度 倒 是 很快   人 挺 多   坐在 沙发 上 感觉 很 舒服']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 20000\n",
    "max_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_feature)\n",
    "tokenizer.fit_on_texts(train_tokens)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_tokens)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = [len(token) for token in train_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(total_num,bins = np.arange(0,410,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7596,\n",
       " 7492,\n",
       " 25,\n",
       " 1,\n",
       " 13692,\n",
       " 148,\n",
       " 2,\n",
       " 144,\n",
       " 89,\n",
       " 1,\n",
       " 271,\n",
       " 100,\n",
       " 802,\n",
       " 2,\n",
       " 142,\n",
       " 9,\n",
       " 2641,\n",
       " 51,\n",
       " 13692,\n",
       " 3,\n",
       " 315,\n",
       " 79,\n",
       " 147,\n",
       " 144,\n",
       " 89,\n",
       " 56,\n",
       " 2,\n",
       " 10,\n",
       " 51,\n",
       " 32,\n",
       " 7080,\n",
       " 1,\n",
       " 417,\n",
       " 98,\n",
       " 459,\n",
       " 13692,\n",
       " 3,\n",
       " 92,\n",
       " 3495,\n",
       " 1087,\n",
       " 81,\n",
       " 1,\n",
       " 126,\n",
       " 1195,\n",
       " 582,\n",
       " 1,\n",
       " 1021,\n",
       " 3,\n",
       " 989,\n",
       " 1,\n",
       " 91,\n",
       " 2446,\n",
       " 2412,\n",
       " 2098,\n",
       " 1789,\n",
       " 3521,\n",
       " 2,\n",
       " 14,\n",
       " 19,\n",
       " 181,\n",
       " 14297,\n",
       " 19,\n",
       " 181,\n",
       " 7688,\n",
       " 10,\n",
       " 3,\n",
       " 468,\n",
       " 448,\n",
       " 1,\n",
       " 5660,\n",
       " 964,\n",
       " 192,\n",
       " 2,\n",
       " 1058,\n",
       " 56,\n",
       " 5660,\n",
       " 6785,\n",
       " 1,\n",
       " 1021,\n",
       " 88,\n",
       " 2,\n",
       " 1285,\n",
       " 5660,\n",
       " 9,\n",
       " 763,\n",
       " 10,\n",
       " 19,\n",
       " 19218,\n",
       " 2,\n",
       " 22,\n",
       " 8145,\n",
       " 1,\n",
       " 130,\n",
       " 599,\n",
       " 5660,\n",
       " 1,\n",
       " 206,\n",
       " 7,\n",
       " 49,\n",
       " 1824,\n",
       " 36,\n",
       " 21,\n",
       " 1382,\n",
       " 5660,\n",
       " 207,\n",
       " 10,\n",
       " 211,\n",
       " 802,\n",
       " 1,\n",
       " 147,\n",
       " 100,\n",
       " 38,\n",
       " 2,\n",
       " 72,\n",
       " 1520,\n",
       " 18252,\n",
       " 59]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_sequences(train_sequences, maxlen=max_len)\n",
    "valid_x = pad_sequences(valid_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coarse sentiment predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GlobalMaxPool1D, Dropout, BatchNormalization,Bidirectional, GRU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_sentiment_model():\n",
    "    model = Sequential()\n",
    "    embedding_size = 200\n",
    "    model.add(Embedding(max_feature, embedding_size, input_length=max_len))\n",
    "    #model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(6, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "coarse_model = coarse_sentiment_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 94500 samples, validate on 10500 samples\n",
      "Epoch 1/2\n",
      "94500/94500 [==============================] - 177s 2ms/step - loss: 0.3779 - acc: 0.8442 - val_loss: 0.2524 - val_acc: 0.9156\n",
      "Epoch 2/2\n",
      "94500/94500 [==============================] - 171s 2ms/step - loss: 0.2442 - acc: 0.9130 - val_loss: 0.2339 - val_acc: 0.9160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5210c49cc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 8s 520us/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = coarse_model.evaluate(valid_x, valid_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.23810461955070494, acc: 0.9144555386225383\n"
     ]
    }
   ],
   "source": [
    "print(\"loss: {}, acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine sentiment predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_sentiment_model(output_len):\n",
    "    embedding_size = 200\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feature, embedding_size, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_len, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义accuarcy的原因\n",
    "\n",
    "首先keras的accuarcy，计算逻辑是：np.mean(np.equal(y_true, y_pred))\n",
    "当y是多标签的时候\n",
    "\n",
    "```\n",
    "         [1, 0, 0]\n",
    "y_true = [1, 0, 0]\n",
    "         [1, 0, 0]\n",
    "         \n",
    "         [0, 1, 0]\n",
    "y_true = [1, 0, 0]\n",
    "         [1, 0, 0]\n",
    "```\n",
    "keras是算每个 $y_{i,j}$是否相同，比如这里只有$y_{0,1}$不同，所以精确度是 $1 - \\frac{1}{9}$\n",
    "\n",
    "而sklearn的accuracy_score是怎么算的呢，\n",
    "它只按每次采样算，即按行算，如果这一行里，只要有一个值不相等，就判断该采样时失败的\n",
    "这里的精确度是 $1 - \\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment_accuarcy(y_true, y_prob):\n",
    "    # 按照选择最大概率的原则，进行分类\n",
    "    np_y_true = y_true\n",
    "    np_y_prob = y_prob\n",
    "    \n",
    "    row, col = np_y_true.shape\n",
    "    \n",
    "    labels = np.zeros((row, col), dtype=int)\n",
    "    \n",
    "    pred_result = np.zeros((row, int(col / 4)), dtype=int)\n",
    "    \n",
    "    for i in range(row):\n",
    "        for j in range(0, col, 4):\n",
    "            max_prob_idx = j + np.argmax(np_y_prob[i, j:j + 4])\n",
    "            labels[i, max_prob_idx] = 1\n",
    "\n",
    "    for i in range(row):\n",
    "        for j in range(0, col, 4):\n",
    "            pred_result[i, int(j / 4)] = np.where(\n",
    "                np.sum(np.equal(np_y_true[i, j:j + 4], labels[i, j:j + 4])) == 4, \n",
    "                1, 0)\n",
    "    \n",
    "    return np.mean(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = self.model.predict(X_val, batch_size=5000)\n",
    "  \n",
    "        print('cus_acc: {}'.format(find_sentiment_accuarcy(y_val, y_predict)))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_y(database, items):\n",
    "    y_items = database[items].values\n",
    "    \n",
    "    # expand [-2, -1, 0, 1] to array index\n",
    "    y_items += 2\n",
    "    \n",
    "    y_fine = np.zeros((len(database), 4 * len(items)))\n",
    "    \n",
    "    for i in range(y_items.shape[0]):\n",
    "        for j in range(y_items.shape[1]):\n",
    "            y_fine[i, j * 4 + y_items[i, j]] = 1\n",
    "    \n",
    "    return y_fine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_loc = get_fine_y(train, location_item)\n",
    "valid_y_loc = get_fine_y(valid, location_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_model = fine_sentiment_model(len(location_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 246s 2ms/step - loss: 0.3286 - acc: 0.8544 - val_loss: 0.1566 - val_acc: 0.9485\n",
      "cus_acc: 0.8952222222222223\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 243s 2ms/step - loss: 0.1497 - acc: 0.9478 - val_loss: 0.1415 - val_acc: 0.9523\n",
      "cus_acc: 0.9042444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5159531860>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_model.fit(train_x, train_y_loc, \n",
    "              batch_size=batch_size, epochs=epochs, \n",
    "              validation_data=(valid_x, valid_y_loc), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 10s 675us/step\n",
      "loss: 0.13337093387444815, acc: 0.9521444370269775\n"
     ]
    }
   ],
   "source": [
    "#loss, acc = loc_model.evaluate(valid_x, valid_y_fine_loc, batch_size=batch_size)\n",
    "#print(\"loss: {}, acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_y = loc_model.predict(valid_x[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## service classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_model = fine_sentiment_model(len(service_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_service = get_fine_y(train, service_item)\n",
    "valid_y_service = get_fine_y(valid, service_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 245s 2ms/step - loss: 0.3286 - acc: 0.8667 - val_loss: 0.1503 - val_acc: 0.9458\n",
      "cus_acc: 0.8888166666666667\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 242s 2ms/step - loss: 0.1522 - acc: 0.9453 - val_loss: 0.1338 - val_acc: 0.9510\n",
      "cus_acc: 0.8986833333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0e8152ef0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_model.fit(train_x, train_y_service, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_service), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_y = service_model.predict(valid_x, batch_size=5000, verbose=1)\n",
    "#predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model = fine_sentiment_model(len(price_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_price = get_fine_y(train, price_item)\n",
    "valid_y_price = get_fine_y(valid, price_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 245s 2ms/step - loss: 0.4221 - acc: 0.7977 - val_loss: 0.2411 - val_acc: 0.9022\n",
      "cus_acc: 0.8001555555555555\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 243s 2ms/step - loss: 0.2330 - acc: 0.9048 - val_loss: 0.2139 - val_acc: 0.9118\n",
      "cus_acc: 0.8208666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0e8152f28>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_model.fit(train_x, train_y_price, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_price), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = fine_sentiment_model(len(environment_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_env = get_fine_y(train, environment_item)\n",
    "valid_y_env = get_fine_y(valid, environment_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 248s 2ms/step - loss: 0.4105 - acc: 0.8065 - val_loss: 0.2417 - val_acc: 0.9032\n",
      "cus_acc: 0.7990666666666667\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 243s 2ms/step - loss: 0.2364 - acc: 0.9073 - val_loss: 0.2104 - val_acc: 0.9172\n",
      "cus_acc: 0.8305666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0bcf7e588>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_model.fit(train_x, train_y_env, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_env), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dish_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_model = fine_sentiment_model(len(dish_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_dish = get_fine_y(train, dish_item)\n",
    "valid_y_dish = get_fine_y(valid, dish_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 247s 2ms/step - loss: 0.4355 - acc: 0.7953 - val_loss: 0.3153 - val_acc: 0.8745\n",
      "cus_acc: 0.7485833333333334\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 244s 2ms/step - loss: 0.2770 - acc: 0.8892 - val_loss: 0.2810 - val_acc: 0.8919\n",
      "cus_acc: 0.7802166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0aaee58d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_model.fit(train_x, train_y_dish, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_dish), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_model = fine_sentiment_model(len(others_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_others = get_fine_y(train, others_item)\n",
    "valid_y_others = get_fine_y(valid, others_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 248s 2ms/step - loss: 0.3798 - acc: 0.8255 - val_loss: 0.2248 - val_acc: 0.9096\n",
      "cus_acc: 0.8164333333333333\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 243s 2ms/step - loss: 0.2212 - acc: 0.9126 - val_loss: 0.2154 - val_acc: 0.9117\n",
      "cus_acc: 0.8226333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0a8f4ba58>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others_model.fit(train_x, train_y_others, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_others), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all sentiment item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_item = location_item + service_item + price_item + environment_item + dish_item + others_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_all = get_fine_y(train, all_item)\n",
    "valid_y_all = get_fine_y(valid, all_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model = fine_sentiment_model(len(all_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 251s 2ms/step - loss: 0.4691 - acc: 0.7729 - val_loss: 0.3093 - val_acc: 0.8750\n",
      "cus_acc: 0.74238\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 247s 2ms/step - loss: 0.2951 - acc: 0.8801 - val_loss: 0.2581 - val_acc: 0.8964\n",
      "cus_acc: 0.7846033333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5120a03518>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model.fit(train_x, train_y_all, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x, valid_y_all), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stopwords():\n",
    "    stopwords = []\n",
    "    with open('baidu_stopwords.txt', 'r') as f:\n",
    "        word = f.readline()\n",
    "        while word:\n",
    "            stopwords.append(word)\n",
    "            word = ''.join(f.readline().split())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1395"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = prepare_stopwords()\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token):\n",
    "    ret = []\n",
    "    for t in token.split():\n",
    "        if t in stopwords:\n",
    "            continue\n",
    "        ret.append(t)\n",
    "    return ' '.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_stopwords_tokens = list(map(remove_stopwords, train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_no_stopwords_tokens = list(map(remove_stopwords, valid_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_feature)\n",
    "tokenizer.fit_on_texts(train_no_stopwords_tokens)\n",
    "\n",
    "train_sequences_ns = tokenizer.texts_to_sequences(train_no_stopwords_tokens)\n",
    "valid_sequences_ns = tokenizer.texts_to_sequences(valid_no_stopwords_tokens)\n",
    "\n",
    "train_x_ns = pad_sequences(train_sequences_ns, maxlen=max_len)\n",
    "valid_x_ns = pad_sequences(valid_sequences_ns, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(output_len):\n",
    "    embedding_size = 200\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feature, embedding_size, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_len, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/lin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "all_model = fine_sentiment_model(len(all_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 253s 2ms/step - loss: 0.4526 - acc: 0.7853 - val_loss: 0.2963 - val_acc: 0.8801\n",
      "cus_acc: 0.7544666666666666\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 249s 2ms/step - loss: 0.2912 - acc: 0.8823 - val_loss: 0.2553 - val_acc: 0.8981\n",
      "cus_acc: 0.7887566666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51120657f0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model.fit(train_x_ns, train_y_all, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x_ns, valid_y_all), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = test_model(len(all_item) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 245s 2ms/step - loss: 0.4137 - acc: 0.8169 - val_loss: 0.2813 - val_acc: 0.8868\n",
      "cus_acc: 0.7659966666666667\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 242s 2ms/step - loss: 0.2732 - acc: 0.8913 - val_loss: 0.2433 - val_acc: 0.9020\n",
      "cus_acc: 0.7969666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f092d1cb390>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.fit(train_x_ns, train_y_all, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x_ns, valid_y_all), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load('vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_word_vector_weight(one_hot_tokenizer, word_model, num_words, dim):\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    \n",
    "    word_index = one_hot_tokenizer.word_index\n",
    "    \n",
    "    for word, idx in word_index.items():\n",
    "        if idx >= num_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_model.wv[word]\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        except KeyError:\n",
    "            #print(\"exceptino {} {}\".format(word, idx))\n",
    "            embedding_matrix[idx]=np.random.normal(0, np.sqrt(0.25), dim)\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-3.64662671,  2.06254768,  0.91882145, ..., -0.95791441,\n",
       "        -0.03822494,  0.00672168],\n",
       "       [-0.18510048,  1.51968443,  0.18783356, ..., -1.41595054,\n",
       "         2.24965906, -0.54286265],\n",
       "       ...,\n",
       "       [-0.09556194,  0.47229677,  0.23177828, ...,  0.08421881,\n",
       "        -0.36064297,  0.28705683],\n",
       "       [-0.05774608,  0.51059407, -0.09923248, ..., -0.22529501,\n",
       "        -0.08396143, -0.3560538 ],\n",
       "       [-0.40956119,  0.10312871,  0.33381173, ...,  0.45765695,\n",
       "        -0.55587614, -0.02540402]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weight = prepare_word_vector_weight(tokenizer, w2v_model, max_feature, 200)\n",
    "word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weight_model(output_len, weight):\n",
    "    embedding_size = 200\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feature, embedding_size, weights=[weight], input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_len, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = test_weight_model(len(all_item) * 4, word_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 245s 2ms/step - loss: 0.4136 - acc: 0.8130 - val_loss: 0.2860 - val_acc: 0.8859\n",
      "cus_acc: 0.7618266666666667\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 242s 2ms/step - loss: 0.2730 - acc: 0.8922 - val_loss: 0.2377 - val_acc: 0.9055\n",
      "cus_acc: 0.8024233333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08e2e717b8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.fit(train_x_ns, train_y_all, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(valid_x_ns, valid_y_all), callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "基于valid的测试，精度为 0.8024233333333334\n",
    "\n",
    "经过多轮测试和优化，发现最好的模型是：test_weight_model\n",
    "\n",
    "增大Dropout，可以显著减少过拟合\n",
    "\n",
    "另外，词向量使用的是开源语料库，地址：https://github.com/SophonPlus/ChineseNlpCorpus\n",
    "\n",
    "使用了其中的数据yf_dianping，100万条评论用于训练词向量\n",
    "\n",
    "基本流程为：\n",
    "\n",
    "* 过滤停止词\n",
    "* 导入词向量\n",
    "* 一层Embedding\n",
    "* 一层双向LSTM(试了GRP)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
